---
title: "Research Projects"
permalink: /projects/
author_profile: true
---
<table style="width:850px;border:none;border-spacing:0px;border-collapse:separate;font-size:90%">
    <tbody>
        <tr style="border-spacing:none;margin-right:auto;margin-left:auto;margin-bottom:0px;margin-top:0px">
            <td style="width:27.5%;vertical-align:top;border:none;min-width:120px">
                <br>
              <img src="/files/climb.jpg" alt="climb image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="width:75%;vertical-align:text-top;border:none">
              <h2>CLiMB: The Continual Learning in Multimodality Benchmark</h2>
              <strong>Collaborators:</strong> Ting-Yun Chang, Leticia Pinto Alva
              <br>
              <strong>Advisors:</strong> Jesse Thomason, Mohammad Rostami
              <br>
              <p>CLiMB is a benchmark to study the novel challenge of learning multimodal tasks in a continual learning setting. Models in CLiMB are systematically evaluated on their ability to learn a sequence of vision-langauge tasks, and downstream low-shot transfer to new multimodal and unimodal tasks.</p>
              <p><a href="https://arxiv.org/abs/2206.09059" class="btn" style="text-decoration: none" target="_blank">arXiV Preprint</a>  
                <a href="https://github.com/GLAMOR-USC/CLiMB" class="btn" style="text-decoration: none" target="_blank">Code</a> 
              </p>
            </td>
        </tr>
        <tr style="border-spacing:none;margin-right:auto;margin-left:auto;margin-bottom:0px;margin-top:0px">
            <td style="width:27.5%;vertical-align:top;border:none;min-width:120px">
                <br>
                <br>
              <img src="/files/wobw.png" alt="wobw image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="width:75%;vertical-align:text-top;border:none">
              <h2>Gender Bias in Pre-Trained Vision-and-Language Models</h2>
              <strong>Advisor :</strong> Yonatan Bisk
              <br>
              <p>We extend text-based bias analysis methods to investigate multimodal language models, and analyze intra- and inter-modality associations and biases learned by these models. Specifically, we demonstrate that <a href="https://arxiv.org/abs/1908.08530">VL-BERT</a> exhibits gender biases, often preferring to reinforce a stereotype over faithfully describing the visual scene.</p>
              <p><a href="https://arxiv.org/abs/2104.08666" class="btn" style="text-decoration: none" target="_blank">GeB4NLP 2022</a></p>
            </td>
        </tr>
        <tr style="border-spacing:none;margin-right:auto;margin-left:auto;margin-bottom:0px;margin-top:0px">
            <td style="width:27.5%;vertical-align:top;border:none;min-width:120px">
                <br>
                <br>
              <img src="/files/masr.png" alt="M-ASR image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="width:75%;vertical-align:text-top;border:none">
              <h2>Multimodal ASR for Recovering Noisy and Corrupted Speech</h2>
              <strong>Collaborator :</strong> Ramon Sanabria
              <br>
              <strong>Advisors:</strong> Desmond Elliott, Florian Metze
              <br>
              <p>We present the first work on the utility of multimodal ASR <em>under noisy conditions</em>, showing that the visual context can be leveraged to recover masked words in the speech signal. We show that fine-grained visual information can improve grounding of missing speech, even under generalized noisy conditions.</p>
              <p><a href="https://arxiv.org/abs/2002.05639" class="btn" style="text-decoration: none" target="_blank">ICASSP 2020</a>
                <a href="https://arxiv.org/abs/2010.02384" class="btn" style="text-decoration: none" target="_blank">Findings of EMNLP 2020</a>
                <a href="https://arxiv.org/abs/2010.08642" class="btn" style="text-decoration: none" target="_blank">NLPBT 2020</a>
                <a href="https://github.com/tejas1995/MultimodalASR/" class="btn" style="text-decoration: none" target="_blank">Code</a> 
              </p>
            </td>
        </tr>
          

          
    </tbody>
</table>