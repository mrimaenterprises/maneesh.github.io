---
title: "Research Projects"
permalink: /projects/
author_profile: true
---
<table style="width:850px;border:none;border-spacing:0px;border-collapse:separate;font-size:90%">
    <tbody>
        <tr style="border-spacing:none;margin-right:auto;margin-left:auto;margin-bottom:0px;margin-top:0px">
            <td style="width:27.5%;vertical-align:middle;border:none;min-width:120px">
              <img src="/files/climb.jpg" alt="climb image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="width:75%;vertical-align:text-top;border:none">
              <h2>CLiMB: The Continual Learning in Multimodality Benchmark</h2>
              <strong>Collaborators:</strong> Ting-Yun Chang, Leticia Pinto Alva
              <br>
              <strong>Advisors:</strong> Jesse Thomason, Mohammad Rostami
              <br>
              <p>CLiMB is a benchmark to study the novel challenge of learning vision-and-language tasks in a continual learning setting. </p>
              <p><a href="https://arxiv.org/abs/2206.09059" class="btn" target="_blank">NeurIPS Datasets and Benchmarks 2022</a>  
                <a href="https://github.com/GLAMOR-USC/CLiMB" class="btn" target="_blank">Code</a> <a href="/files/climb-slides.pdf" class="btn" target="_blank">Slides</a> <a href="https://www.youtube.com/watch?v=zkw2S3TWJA0&list=PLm6QXeaB-XkBMFxvgZvYjqhaPgGg8Um9Z" class="btn" target="_blank">Video</a>
              </p>
            </td>
        </tr>
        <tr style="border-spacing:none;margin-right:auto;margin-left:auto;margin-bottom:0px;margin-top:0px">
            <td style="width:27.5%;vertical-align:middle;border:none;min-width:120px">
                <br>
                <br>
              <img src="/files/wobw.png" alt="wobw image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="width:75%;vertical-align:text-top;border:none">
              <h2>Gender Bias in Pre-Trained Vision-and-Language Models</h2>
              <strong>Advisor :</strong> Yonatan Bisk
              <br>
              <p>We analyze intra- and inter-modality gender biases encoded by pre-trained vision-and-language models, which often prefer to reinforce stereotypes over faithfully describing the visual scene.</p>
              <p><a href="https://arxiv.org/abs/2104.08666" class="btn" target="_blank">GeB4NLP 2022</a></p>
            </td>
        </tr>
        <tr style="border-spacing:none;margin-right:auto;margin-left:auto;margin-bottom:0px;margin-top:0px">
            <td style="width:27.5%;vertical-align:middle;border:none;min-width:120px">
                <br>
              <img src="/files/masr.png" alt="M-ASR image" style="width:auto; height:auto; max-width:100%;">
            </td>
            <td style="width:75%;vertical-align:text-top;border:none">
              <h2>Multimodal ASR for Recovering Noisy and Corrupted Speech</h2>
              <strong>Collaborator :</strong> Ramon Sanabria
              <br>
              <strong>Advisors:</strong> Desmond Elliott, Florian Metze
              <br>
              <p>We investigate the utility of multimodal ASR <em>under noisy conditions</em>, showing that the visual context can be leveraged to recover masked words in the speech signal. </p>
              <p><a href="https://arxiv.org/abs/2002.05639" class="btn" target="_blank">ICASSP 2020</a>
                <a href="https://arxiv.org/abs/2010.02384" class="btn" target="_blank">Findings of EMNLP 2020</a>
                <a href="https://arxiv.org/abs/2010.08642" class="btn" target="_blank">NLPBT 2020</a>
                <a href="https://github.com/tejas1995/MultimodalASR/" class="btn" target="_blank">Code</a> 
              </p>
            </td>
        </tr>
          

          
    </tbody>
</table>